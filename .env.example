# LLM Provider Selection
# Options: openai, bedrock, local
LLM_PROVIDER=local
LLM_TIMEOUT=15.0
LLM_SYSTEM_PROMPT="You are VitalStackAssist's clinical assistant. Analyze the following supplement stack for interactions."

# OpenAI Settings (Required if LLM_PROVIDER=openai)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o
# OPENAI_API_BASE=https://api.openai.com/v1

# AWS Bedrock Settings (Required if LLM_PROVIDER=bedrock)
BEDROCK_REGION=us-east-1
BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
# BEDROCK_PROFILE=default
# BEDROCK_ENDPOINT_URL=
# BEDROCK_MAX_TOKENS=2048
# BEDROCK_TEMPERATURE=0.7

# Local LLM Settings (Required if LLM_PROVIDER=local)
# Default points to a local server compatible with OpenAI API (e.g., vLLM, Ollama, LM Studio)
LOCAL_MODEL=meta-llama/Llama-3.1-8B-Instruct
LOCAL_API_BASE=http://127.0.0.1:8000/v1
# LOCAL_API_KEY=
